Introduction to Deep learning (6.S191)
Loss Quantification 

 The measure of our neural network the cost incurred from the incorrect predications .
The actual and the predicated values must be as close  as possible ,  so that there is the less lose that is experienced by the NN . 

what is the empirical Loss ? 
This is the measure of the loss over our entire dataset . 
This is given by maths function  :

J = 1/n (summation i=1 to n (L(f(x) I ,W) , yi)

This J function is called as the Objective function  , cost function or the empirical risk .
The objective of the NN is to reduce the Empirical Loss  in the dataset to get the best predication from our neural network . 


Binary cross entropy Loss (Log loss)
Cross entropy loss can be used with the model that outputs a probability of 0 and 1 / between the zero and the One.  

SoftMax function 
This is use to convert the Raw data predication mainly  the logits into probability value / distribution  . These are sort of the activation functions , Logits more officially are  :

Logits, also known as raw scores, are the output of a neural network before the SoftMax activation function is applied. They represent the unnormalized predictions generated by the network for each class in a classification problem .

More detailed over view on the two are  :
Logits: The term "logits" refers to the raw, unnormalized predictions made by the neural network. These predictions are typically obtained from the last layer of the network, which could be a fully connected layer or a convolutional layer, depending on the architecture.

Softmax Activation: Logits are often passed through a softmax activation function to convert them into probabilities. The softmax function ensures that the output is a valid probability distribution, where each value represents the probability of the corresponding class.


Loss Optimization 
WE wish to find the network Weights W that achieve the Lowest Loss . 
W(star) = arg min(1/n(summation I = 1 to I =  n L (f(xi,w),yi) 

W star  = arg min J(w) 
W is just a list of the number for the every single neuron and the layer of the neuron . This may be valued for the feedforward or maybe for the  CNN neural network . 

This finding of the w helps in the reduction  of the loss and the otpmisation  for the loss overall .


> The loss function is a simple function  of the Weights , similar to the one in the activation function and the stuff .  
If there are all in all two weight then the activation function is basicvally a two valued weight function . 
W not , w1 -> J(wnot , W1) .

Gradient Computation 
Gradient -> Diff(J(w))/diff(W)

Gradient Descendent Algorithm -> For the loss optimisation 
Step1 : Initialize the weights randomly ~ N(0, sigma**2))
Step 2  : Loop until convergence  .
step 3 : Compute the gradient 
step 4  : Update the weights that are W <- W - eta(gradient)
also the 
gradient is =  d(j(W))/d(w)

step 5 : return the weights 

Neural Network in the practice :
Training is very difficult in the reality . 
The gradient descend algo is used in the optimisation  . 
Also the eta un the above is set by the user , which is basically one of the toughest things .

The eta is the learning rates  that has to be set by the user   .
eta is the function of the gradient and the eta is never a constant and it can be high  / low . 


what are the overall step for the optimisation and the train of the neural network  ?
The overall steps are  :;

1. choose the optimized from the TensorFlow keras 
2.make the algo for the gradient descent 
3.use the W <- W - eta *gradient desc) to predict the weights , the most optimised weight leads to the loss optimisation  . 

The above normal gradient descent algo  is very  expensive computationally . 

To optimise the gradient descent algo we use the Scholiastic gradient descent algo  :
called as the SGD : 

SGD  scholastic gradient descent algo 
The main difference in   the gradient and  the SGd is that the SGD takes only one training point or the data set and then find the gradient for that repeatedly  , this is computationally less expensive  .
The SGd is overall based on the Mini batch that is taking  small batches of the data to compute the gradient and optimise  the weights . 

Take THE SUBSET  that is basically a smaller set , of the whole data set  . The computation is very much . The convergence to the output is very  fast here . 


Overfitting : Too complex , extra parameter and does not generalize well  . This is basic problem ,  does have to capacity to define  if the model captures the output correctly  or not .  


Ideal fit  : training is done properly . the model works fine .
Underfit :
Model does not  have  the capacity  to learnt fully  the data .   The model is not capable . 


Regularization : Technique  , that constraint the optimisation problem to discoing the complex  models . The nuances are not produced in the data model  .

Regularization  Methods  :
1.Dropout randomly set some  of the activation to 0 . 
typically drop the 50% of the data in the activation in the layer  to 0 
Forces the data to not relay on any one node . 

TensorFlow syntax  :
tf.keras.layers.Dropout(p=0.5) ideally the best to have the 50 % red . 

2.Reaply on  any particular  layer in the Nn can lead to Over/ under fit 
3. Early stopping  
when the  testing and the training data point coincide then stop at that particular point ,  as this is the best point for the data train and the testing  above this can lead to under and the over fit , 



Backpropagation is the method to optimise the NN. 
Training is done :
1.batching 
2.adaPTIVE Learning 
3.Regulaiztion
